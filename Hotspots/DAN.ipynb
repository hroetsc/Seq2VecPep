{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function keras.backend.tensorflow_backend.set_session(session)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import keras.backend.tensorflow_backend as K\n",
    "K.set_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 104)\n"
     ]
    }
   ],
   "source": [
    "### INPUT ###\n",
    "# load sequence representation and labels\n",
    "# extended substring\n",
    "# seq2vec + TFIDF\n",
    "seq_repres = pd.read_csv('ext_substr_w5_d100_seq2vec-TFIDF.csv')\n",
    "\n",
    "print(seq_repres.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN PART ###\n",
    "# input vector: row names\n",
    "X = np.array(seq_repres.index, dtype = 'int64')\n",
    "\n",
    "# embeddings initialzer\n",
    "embedding_matrix = np.array(seq_repres.iloc[:,4:104], dtype = 'float64')\n",
    "\n",
    "# label vector\n",
    "labels = np.array(seq_repres['label'], dtype = 'object')\n",
    "labels = np.where(labels == 'hotspot', 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build DAN (will terribly overfit)\n",
    "\n",
    "embeddingDim = 100\n",
    "\n",
    "\n",
    "def build_and_compile_model():\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # imput layer\n",
    "    input_l = keras.Input(shape = (1,), name = 'input_layer')\n",
    "    \n",
    "    # initialize with seq2vec-representation\n",
    "    embedding = layers.Embedding(input_dim = len(X),\n",
    "                                output_dim = embeddingDim,\n",
    "                                input_length = 1,\n",
    "                                weights = [embedding_matrix],\n",
    "                                trainable = True,\n",
    "                                name = 'embedding')(input_l)\n",
    "    \n",
    "    dense1 = layers.Dense(32, activation = 'tanh')(embedding)\n",
    "    norm1 = layers.BatchNormalization(trainable = True)(dense1)\n",
    "    drop1 = layers.Dropout(0.3)(norm1)\n",
    "    \n",
    "    dense4 = layers.Dense(8, activation = 'relu')(drop1)\n",
    "    norm4 = layers.BatchNormalization(trainable = True)(dense4)\n",
    "    \n",
    "    output = layers.Dense(1, activation = 'sigmoid')(norm4)\n",
    "    \n",
    "    \n",
    "    # model\n",
    "    model = keras.Model(inputs=input_l, outputs=output)\n",
    "    opt = keras.optimizers.Adagrad()\n",
    "    \n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "                    optimizer = opt,\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1, 100)            1000000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1, 32)             3232      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 1, 32)             128       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 8)              264       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 1, 8)              32        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 1)              9         \n",
      "=================================================================\n",
      "Total params: 1,003,665\n",
      "Trainable params: 1,003,585\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_and_compile_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "10000/10000 [==============================] - 2s 205us/sample - loss: 0.0042 - acc: 0.9993\n",
      "Epoch 2/30\n",
      "10000/10000 [==============================] - 2s 181us/sample - loss: 2.8981e-04 - acc: 1.0000\n",
      "Epoch 3/30\n",
      "10000/10000 [==============================] - 2s 183us/sample - loss: 1.4497e-04 - acc: 1.0000\n",
      "Epoch 4/30\n",
      "10000/10000 [==============================] - 2s 183us/sample - loss: 1.0036e-04 - acc: 1.0000\n",
      "Epoch 5/30\n",
      "10000/10000 [==============================] - 2s 182us/sample - loss: 7.6180e-05 - acc: 1.0000\n",
      "Epoch 6/30\n",
      "10000/10000 [==============================] - 2s 179us/sample - loss: 6.2092e-05 - acc: 1.0000\n",
      "Epoch 7/30\n",
      "10000/10000 [==============================] - 2s 184us/sample - loss: 5.1135e-05 - acc: 1.0000\n",
      "Epoch 8/30\n",
      "10000/10000 [==============================] - 2s 184us/sample - loss: 4.3523e-05 - acc: 1.0000\n",
      "Epoch 9/30\n",
      "10000/10000 [==============================] - 2s 183us/sample - loss: 3.8472e-05 - acc: 1.0000\n",
      "Epoch 10/30\n",
      "10000/10000 [==============================] - 2s 181us/sample - loss: 3.4412e-05 - acc: 1.0000\n",
      "Epoch 11/30\n",
      "10000/10000 [==============================] - 2s 183us/sample - loss: 3.1146e-05 - acc: 1.0000\n",
      "Epoch 12/30\n",
      "10000/10000 [==============================] - 2s 186us/sample - loss: 2.8374e-05 - acc: 1.0000\n",
      "Epoch 13/30\n",
      "10000/10000 [==============================] - 2s 189us/sample - loss: 2.6083e-05 - acc: 1.0000\n",
      "Epoch 14/30\n",
      "10000/10000 [==============================] - 2s 187us/sample - loss: 2.4125e-05 - acc: 1.0000\n",
      "Epoch 15/30\n",
      "10000/10000 [==============================] - 2s 187us/sample - loss: 2.2430e-05 - acc: 1.0000\n",
      "Epoch 16/30\n",
      "10000/10000 [==============================] - 2s 184us/sample - loss: 2.0728e-05 - acc: 1.0000\n",
      "Epoch 17/30\n",
      "10000/10000 [==============================] - 2s 185us/sample - loss: 1.9244e-05 - acc: 1.0000\n",
      "Epoch 18/30\n",
      "10000/10000 [==============================] - 2s 186us/sample - loss: 1.8093e-05 - acc: 1.0000\n",
      "Epoch 19/30\n",
      "10000/10000 [==============================] - 2s 185us/sample - loss: 1.7066e-05 - acc: 1.0000\n",
      "Epoch 20/30\n",
      "10000/10000 [==============================] - 2s 185us/sample - loss: 1.6140e-05 - acc: 1.0000\n",
      "Epoch 21/30\n",
      "10000/10000 [==============================] - 2s 187us/sample - loss: 1.5305e-05 - acc: 1.0000\n",
      "Epoch 22/30\n",
      "10000/10000 [==============================] - 2s 193us/sample - loss: 1.4546e-05 - acc: 1.0000\n",
      "Epoch 23/30\n",
      "10000/10000 [==============================] - 2s 196us/sample - loss: 1.3857e-05 - acc: 1.0000\n",
      "Epoch 24/30\n",
      "10000/10000 [==============================] - 2s 185us/sample - loss: 1.3205e-05 - acc: 1.0000\n",
      "Epoch 25/30\n",
      "10000/10000 [==============================] - 2s 186us/sample - loss: 1.2613e-05 - acc: 1.0000\n",
      "Epoch 26/30\n",
      "10000/10000 [==============================] - 2s 187us/sample - loss: 1.2082e-05 - acc: 1.0000\n",
      "Epoch 27/30\n",
      "10000/10000 [==============================] - 2s 183us/sample - loss: 1.1583e-05 - acc: 1.0000\n",
      "Epoch 28/30\n",
      "10000/10000 [==============================] - 2s 185us/sample - loss: 1.1109e-05 - acc: 1.0000\n",
      "Epoch 29/30\n",
      "10000/10000 [==============================] - 2s 186us/sample - loss: 1.0687e-05 - acc: 1.0000\n",
      "Epoch 30/30\n",
      "10000/10000 [==============================] - 2s 185us/sample - loss: 1.0294e-05 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "fit = model.fit([X, labels],\n",
    "                epochs = 30,\n",
    "                verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights and metrics\n",
    "model.save_weights('DAN/weights_ext_seq2vec-TFIDF.h5')\n",
    "\n",
    "val = []\n",
    "name = list(fit.history.keys())\n",
    "for i, elem in enumerate(fit.history.keys()):\n",
    "    val.append(fit.history[elem])\n",
    "\n",
    "m = list(zip(name, val))\n",
    "m = pd.DataFrame(m)\n",
    "pd.DataFrame.to_csv(m, 'DAN/metrics_ext_seq2vec-TFIDF.csv', header=False, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
