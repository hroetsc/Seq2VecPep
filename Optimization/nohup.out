Building DAG of jobs...
Creating conda environment src/environment_base.yml...
Downloading and installing remote packages.
Environment for src/environment_base.yml created (location: .snakemake/conda/c9bf1e46)
Removing incomplete Conda environment src/environment_seq2vec.yml...
Creating conda environment src/environment_seq2vec.yml...
Downloading and installing remote packages.
Environment for src/environment_seq2vec.yml created (location: .snakemake/conda/e14dae6a)
Creating conda environment src/R_dependencies.yml...
Downloading and installing remote packages.
Environment for src/R_dependencies.yml created (location: .snakemake/conda/1c290a20)
Using shell: /bin/bash
Provided cores: 12
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	BPE_training
	1	all
	1	generate_tokens
	1	model_hyperopt
	1	seq2vec_skip_grams
	5

[Fri Mar 27 15:19:10 2020]
rule BPE_training:
    input: data/concatenated_UniProt.txt
    output: data/BPE_model.bpe
    jobid: 0

Activating conda environment: /home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/1c290a20
[1] "### TRAINING OF BYTE-PAIR ENCODING MODEL ###"

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘seqinr’

The following object is masked from ‘package:dplyr’:

    count


Attaching package: ‘berryFunctions’

The following objects are masked from ‘package:seqinr’:

    circle, getName

The following object is masked from ‘package:dplyr’:

    between

Training parameters
  input: data/concatenated_UniProt.txt
  model: data/BPE_model.bpe
  vocab_size: 5000
  n_threads: 8
  character_coverage: 0.999
  pad: 0
  unk: 1
  bos: 2
  eos: 3

reading file...
learning bpe...
number of unique characters in the training data: 27
number of deleted characters: 7
number of unique characters left: 20
id: 1000=33+15                freq: 2537        subword: AAD=AA+D
id: 2000=21+36                freq: 1056        subword: HEK=H+EK
id: 3000=96+21                freq: 578         subword: FVH=FV+H
id: 4000=37+62                freq: 409         subword: RLSK=RL+SK
model saved to: data/BPE_model.bpe
[Fri Mar 27 15:19:23 2020]
Finished job 0.
1 of 5 steps (20%) done

[Fri Mar 27 15:19:23 2020]
rule generate_tokens:
    input: data/formatted_proteome.csv, data/BPE_model.bpe
    output: results/model_vocab.csv, results/words.csv
    jobid: 4

Activating conda environment: /home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/1c290a20
[1] "### TOKENIZATION OF PROTEOME USING BYTE-PAIR ENCODING ALGORITHM ###"

Attaching package: ‘dplyr’

The following objects are masked from ‘package:plyr’:

    arrange, count, desc, failwith, id, mutate, rename, summarise,
    summarize

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘seqinr’

The following object is masked from ‘package:dplyr’:

    count

The following object is masked from ‘package:plyr’:

    count


Attaching package: ‘berryFunctions’

The following objects are masked from ‘package:seqinr’:

    circle, getName

The following object is masked from ‘package:dplyr’:

    between

[1] "PEPTIDE-PAIR ENCODING"
  |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |==                                                                    |   4%  |                                                                              |===                                                                   |   4%  |                                                                              |===                                                                   |   5%  |                                                                              |====                                                                  |   5%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   6%  |                                                                              |=====                                                                 |   7%  |                                                                              |=====                                                                 |   8%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |   9%  |                                                                              |=======                                                               |  10%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |=========                                                             |  14%  |                                                                              |==========                                                            |  14%  |                                                                              |==========                                                            |  15%  |                                                                              |===========                                                           |  15%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  16%  |                                                                              |============                                                          |  17%  |                                                                              |============                                                          |  18%  |                                                                              |=============                                                         |  18%  |                                                                              |=============                                                         |  19%  |                                                                              |==============                                                        |  19%  |                                                                              |==============                                                        |  20%  |                                                                              |==============                                                        |  21%  |                                                                              |===============                                                       |  21%  |                                                                              |===============                                                       |  22%  |                                                                              |================                                                      |  22%  |                                                                              |================                                                      |  23%  |                                                                              |================                                                      |  24%  |                                                                              |=================                                                     |  24%  |                                                                              |=================                                                     |  25%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  26%  |                                                                              |===================                                                   |  27%  |                                                                              |===================                                                   |  28%  |                                                                              |====================                                                  |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |=====================                                                 |  31%  |                                                                              |======================                                                |  31%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |=======================                                               |  34%  |                                                                              |========================                                              |  34%  |                                                                              |========================                                              |  35%  |                                                                              |=========================                                             |  35%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |==========================                                            |  38%  |                                                                              |===========================                                           |  38%  |                                                                              |===========================                                           |  39%  |                                                                              |============================                                          |  39%  |                                                                              |============================                                          |  40%  |                                                                              |============================                                          |  41%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |==============================                                        |  44%  |                                                                              |===============================                                       |  44%  |                                                                              |===============================                                       |  45%  |                                                                              |================================                                      |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |=================================                                     |  48%  |                                                                              |==================================                                    |  48%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |===================================                                   |  51%  |                                                                              |====================================                                  |  51%  |                                                                              |====================================                                  |  52%  |                                                                              |=====================================                                 |  52%  |                                                                              |=====================================                                 |  53%  |                                                                              |=====================================                                 |  54%  |                                                                              |======================================                                |  54%  |                                                                              |======================================                                |  55%  |                                                                              |=======================================                               |  55%  |                                                                              |=======================================                               |  56%  |                                                                              |========================================                              |  56%  |                                                                              |========================================                              |  57%  |                                                                              |========================================                              |  58%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  59%  |                                                                              |==========================================                            |  60%  |                                                                              |==========================================                            |  61%  |                                                                              |===========================================                           |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  62%  |                                                                              |============================================                          |  63%  |                                                                              |============================================                          |  64%  |                                                                              |=============================================                         |  64%  |                                                                              |=============================================                         |  65%  |                                                                              |==============================================                        |  65%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |===============================================                       |  68%  |                                                                              |================================================                      |  68%  |                                                                              |================================================                      |  69%  |                                                                              |=================================================                     |  69%  |                                                                              |=================================================                     |  70%  |                                                                              |=================================================                     |  71%  |                                                                              |==================================================                    |  71%  |                                                                              |==================================================                    |  72%  |                                                                              |===================================================                   |  72%  |                                                                              |===================================================                   |  73%  |                                                                              |===================================================                   |  74%  |                                                                              |====================================================                  |  74%  |                                                                              |====================================================                  |  75%  |                                                                              |=====================================================                 |  75%  |                                                                              |=====================================================                 |  76%  |                                                                              |======================================================                |  76%  |                                                                              |======================================================                |  77%  |                                                                              |======================================================                |  78%  |                                                                              |=======================================================               |  78%  |                                                                              |=======================================================               |  79%  |                                                                              |========================================================              |  79%  |                                                                              |========================================================              |  80%  |                                                                              |========================================================              |  81%  |                                                                              |=========================================================             |  81%  |                                                                              |=========================================================             |  82%  |                                                                              |==========================================================            |  82%  |                                                                              |==========================================================            |  83%  |                                                                              |==========================================================            |  84%  |                                                                              |===========================================================           |  84%  |                                                                              |===========================================================           |  85%  |                                                                              |============================================================          |  85%  |                                                                              |============================================================          |  86%  |                                                                              |=============================================================         |  86%  |                                                                              |=============================================================         |  87%  |                                                                              |=============================================================         |  88%  |                                                                              |==============================================================        |  88%  |                                                                              |==============================================================        |  89%  |                                                                              |===============================================================       |  89%  |                                                                              |===============================================================       |  90%  |                                                                              |===============================================================       |  91%  |                                                                              |================================================================      |  91%  |                                                                              |================================================================      |  92%  |                                                                              |=================================================================     |  92%  |                                                                              |=================================================================     |  93%  |                                                                              |=================================================================     |  94%  |                                                                              |==================================================================    |  94%  |                                                                              |==================================================================    |  95%  |                                                                              |===================================================================   |  95%  |                                                                              |===================================================================   |  96%  |                                                                              |====================================================================  |  96%  |                                                                              |====================================================================  |  97%  |                                                                              |====================================================================  |  98%  |                                                                              |===================================================================== |  98%  |                                                                              |===================================================================== |  99%  |                                                                              |======================================================================|  99%  |                                                                              |======================================================================| 100%Warning message:
`as_tibble.matrix()` requires a matrix with column names or a `.name_repair` argument. Using compatibility `.name_repair`.
This warning is displayed once per session. 
[1] "FORMAT OUTPUT"
[1] "found 0 of 25205 proteins that consist of only one token and is removing them"
[1] "randomize protein order"
[Fri Mar 27 15:20:02 2020]
Finished job 4.
2 of 5 steps (40%) done

[Fri Mar 27 15:20:02 2020]
rule seq2vec_skip_grams:
    input: results/words.csv
    output: results/skipgrams.txt, results/seq2vec_ids.csv
    jobid: 3

Activating conda environment: /home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/e14dae6a
### PROTEOME EMBEDDING USING SKIP-GRAM NEURAL NETWORK - 1 ###
### PROTEOME EMBEDDING USING SKIP-GRAM NEURAL NETWORK - 1 ###
TEXT PREPROCESSING
CONVERT WORDS TO IDs
number of proteins / batches: 1000
vocabulary size: 4964
GENERATE SKIP-GRAMS
skip-grams are calculated in batches (equivalent to proteins)
Using TensorFlow backend.
done with generating skip-grams
SAVE OUTPUT
[Fri Mar 27 15:20:33 2020]
Finished job 3.
3 of 5 steps (60%) done

[Fri Mar 27 15:20:33 2020]
rule model_hyperopt:
    input: results/skipgrams.txt, results/seq2vec_ids.csv
    output: results/hyperopt_gp.csv, results/hyperopt_gbrt.csv, results/conv_gp.png, results/conv_gbrt.png, results/obj_gp.png, results/obj_gbrt.png
    jobid: 2

Activating conda environment: /home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46
2020-03-27 15:20:37.169578: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2020-03-27 15:20:37.169630: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2020-03-27 15:20:37.169642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-03-27 15:20:39.799944: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-03-27 15:20:39.799965: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2020-03-27 15:20:39.799978: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (l1604-04): /proc/driver/nvidia/version does not exist
2020-03-27 15:20:39.800069: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-03-27 15:20:39.827964: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz
2020-03-27 15:20:39.828503: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f25c141550 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-03-27 15:20:39.828513: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
### PROTEOME EMBEDDING USING SKIP-GRAM NEURAL NETWORK - 2 ###
LOAD DATA
target word vector
[151 151 151 ...  79 125  77]
context word vector
[ 691 3801  197 ... 4791 4170 1127]
label vector (converted 0 to -1)
[ 1  1  1 ... -1 -1 -1]
vocabulary size (number of target word IDs +2): 4966
INITIALIZE HYPERPARAMETER OPTIMIZATION
MODEL GENERATION
Epoch 1/10
 - 389s - loss: 0.6760 - accuracy: 0.5511 - val_loss: 0.1052 - val_accuracy: 0.7158
Epoch 2/10
 - 391s - loss: 0.5965 - accuracy: 0.7054 - val_loss: 0.1430 - val_accuracy: 0.7068
Epoch 3/10
 - 390s - loss: 0.5883 - accuracy: 0.7004 - val_loss: 0.1576 - val_accuracy: 0.7013
Epoch 4/10
 - 391s - loss: 0.5844 - accuracy: 0.6970 - val_loss: 0.1608 - val_accuracy: 0.6982
Epoch 5/10
 - 388s - loss: 0.5820 - accuracy: 0.6951 - val_loss: 0.1626 - val_accuracy: 0.6959
Epoch 6/10
 - 391s - loss: 0.5800 - accuracy: 0.6939 - val_loss: 0.1660 - val_accuracy: 0.6945
Epoch 7/10
 - 388s - loss: 0.5787 - accuracy: 0.6930 - val_loss: 0.1657 - val_accuracy: 0.6932
Epoch 8/10
 - 387s - loss: 0.5776 - accuracy: 0.6925 - val_loss: 0.1652 - val_accuracy: 0.6922
Epoch 9/10
 - 391s - loss: 0.5767 - accuracy: 0.6918 - val_loss: 0.1655 - val_accuracy: 0.6914
Epoch 10/10
 - 388s - loss: 0.5758 - accuracy: 0.6914 - val_loss: 0.1650 - val_accuracy: 0.6908

Accuracy: 69.08%

Epoch 1/10
 - 243s - loss: 0.6834 - accuracy: 0.5421 - val_loss: 0.4018 - val_accuracy: 0.6847
Epoch 2/10
 - 241s - loss: 0.5871 - accuracy: 0.6803 - val_loss: 0.4602 - val_accuracy: 0.6787
Epoch 3/10
 - 241s - loss: 0.5772 - accuracy: 0.6766 - val_loss: 0.4770 - val_accuracy: 0.6748
Epoch 4/10
 - 241s - loss: 0.5734 - accuracy: 0.6741 - val_loss: 0.4871 - val_accuracy: 0.6727
Epoch 5/10
 - 241s - loss: 0.5712 - accuracy: 0.6729 - val_loss: 0.4953 - val_accuracy: 0.6712
Epoch 6/10
 - 241s - loss: 0.5695 - accuracy: 0.6722 - val_loss: 0.4993 - val_accuracy: 0.6703
Epoch 7/10
 - 241s - loss: 0.5683 - accuracy: 0.6714 - val_loss: 0.5054 - val_accuracy: 0.6696
Epoch 8/10
 - 241s - loss: 0.5675 - accuracy: 0.6712 - val_loss: 0.5103 - val_accuracy: 0.6689
Epoch 9/10
 - 241s - loss: 0.5667 - accuracy: 0.6709 - val_loss: 0.5133 - val_accuracy: 0.6685
Epoch 10/10
 - 241s - loss: 0.5660 - accuracy: 0.6709 - val_loss: 0.5154 - val_accuracy: 0.6682

Accuracy: 66.82%

Epoch 1/10
 - 831s - loss: 0.6231 - accuracy: 0.6191 - val_loss: 0.2451 - val_accuracy: 0.6872
Epoch 2/10
 - 833s - loss: 0.5464 - accuracy: 0.6901 - val_loss: 0.2064 - val_accuracy: 0.6748
Epoch 3/10
 - 833s - loss: 0.5220 - accuracy: 0.6972 - val_loss: 0.2023 - val_accuracy: 0.6645
Epoch 4/10
 - 833s - loss: 0.5065 - accuracy: 0.7023 - val_loss: 0.2280 - val_accuracy: 0.6594
Epoch 5/10
 - 834s - loss: 0.4958 - accuracy: 0.7067 - val_loss: 0.2458 - val_accuracy: 0.6573
Epoch 6/10
 - 835s - loss: 0.4876 - accuracy: 0.7104 - val_loss: 0.2570 - val_accuracy: 0.6564
Epoch 7/10
 - 834s - loss: 0.4814 - accuracy: 0.7135 - val_loss: 0.2718 - val_accuracy: 0.6558
Epoch 8/10
 - 834s - loss: 0.4762 - accuracy: 0.7163 - val_loss: 0.2868 - val_accuracy: 0.6556
Epoch 9/10
 - 832s - loss: 0.4718 - accuracy: 0.7188 - val_loss: 0.3012 - val_accuracy: 0.6555
Epoch 10/10
 - 833s - loss: 0.4683 - accuracy: 0.7209 - val_loss: 0.3075 - val_accuracy: 0.6554

Accuracy: 65.54%

Epoch 1/10
 - 1565s - loss: 0.7786 - accuracy: 0.3014 - val_loss: 0.7141 - val_accuracy: 0.4628
Epoch 2/10
 - 1564s - loss: 0.7448 - accuracy: 0.4277 - val_loss: 0.6959 - val_accuracy: 0.4925
Epoch 3/10
 - 1560s - loss: 0.7306 - accuracy: 0.4738 - val_loss: 0.6712 - val_accuracy: 0.5063
Epoch 4/10
 - 1558s - loss: 0.7222 - accuracy: 0.4978 - val_loss: 0.6517 - val_accuracy: 0.5155
Epoch 5/10
 - 1557s - loss: 0.7159 - accuracy: 0.5134 - val_loss: 0.6367 - val_accuracy: 0.5218
Epoch 6/10
 - 1557s - loss: 0.7112 - accuracy: 0.5240 - val_loss: 0.6233 - val_accuracy: 0.5268
Epoch 7/10
 - 1584s - loss: 0.7073 - accuracy: 0.5325 - val_loss: 0.6123 - val_accuracy: 0.5309
Epoch 8/10
 - 1573s - loss: 0.7041 - accuracy: 0.5389 - val_loss: 0.6029 - val_accuracy: 0.5341
Epoch 9/10
 - 1576s - loss: 0.7013 - accuracy: 0.5442 - val_loss: 0.5956 - val_accuracy: 0.5371
Epoch 10/10
 - 1573s - loss: 0.6990 - accuracy: 0.5489 - val_loss: 0.5887 - val_accuracy: 0.5396

Accuracy: 53.96%

Epoch 1/10
 - 1826s - loss: 0.7380 - accuracy: 0.4995 - val_loss: 0.5928 - val_accuracy: 0.5877
Epoch 2/10
 - 1826s - loss: 0.6844 - accuracy: 0.6068 - val_loss: 0.5576 - val_accuracy: 0.6028
Epoch 3/10
 - 1816s - loss: 0.6708 - accuracy: 0.6230 - val_loss: 0.5223 - val_accuracy: 0.6094
Epoch 4/10
 - 1818s - loss: 0.6630 - accuracy: 0.6312 - val_loss: 0.4970 - val_accuracy: 0.6130
Epoch 5/10
 - 1818s - loss: 0.6576 - accuracy: 0.6371 - val_loss: 0.4750 - val_accuracy: 0.6157
Epoch 6/10
 - 1815s - loss: 0.6536 - accuracy: 0.6412 - val_loss: 0.4596 - val_accuracy: 0.6175
Epoch 7/10
 - 1814s - loss: 0.6505 - accuracy: 0.6444 - val_loss: 0.4473 - val_accuracy: 0.6191
Epoch 8/10
 - 1814s - loss: 0.6479 - accuracy: 0.6472 - val_loss: 0.4377 - val_accuracy: 0.6204
Epoch 9/10
 - 1812s - loss: 0.6457 - accuracy: 0.6494 - val_loss: 0.4290 - val_accuracy: 0.6214
Epoch 10/10
 - 1814s - loss: 0.6438 - accuracy: 0.6515 - val_loss: 0.4228 - val_accuracy: 0.6224

Accuracy: 62.24%

Epoch 1/10
 - 1606s - loss: 0.8022 - accuracy: 0.1363 - val_loss: 0.5970 - val_accuracy: 0.1013
Epoch 2/10
 - 1607s - loss: 0.7996 - accuracy: 0.1481 - val_loss: 0.5957 - val_accuracy: 0.1160
Epoch 3/10
 - 1607s - loss: 0.7990 - accuracy: 0.1528 - val_loss: 0.5945 - val_accuracy: 0.1245
Epoch 4/10
 - 1607s - loss: 0.7984 - accuracy: 0.1569 - val_loss: 0.5937 - val_accuracy: 0.1305
Epoch 5/10
 - 1607s - loss: 0.7981 - accuracy: 0.1596 - val_loss: 0.5929 - val_accuracy: 0.1351
Epoch 6/10
 - 1607s - loss: 0.7979 - accuracy: 0.1619 - val_loss: 0.5923 - val_accuracy: 0.1388
Epoch 7/10
 - 1606s - loss: 0.7976 - accuracy: 0.1642 - val_loss: 0.5917 - val_accuracy: 0.1419
Epoch 8/10
 - 1607s - loss: 0.7974 - accuracy: 0.1657 - val_loss: 0.5913 - val_accuracy: 0.1446
Epoch 9/10
 - 1606s - loss: 0.7972 - accuracy: 0.1673 - val_loss: 0.5908 - val_accuracy: 0.1470
Epoch 10/10
 - 1607s - loss: 0.7970 - accuracy: 0.1687 - val_loss: 0.5903 - val_accuracy: 0.1492

Accuracy: 14.92%

Epoch 1/10
 - 918s - loss: 0.7275 - accuracy: 0.5072 - val_loss: 0.2297 - val_accuracy: 0.6194
Epoch 2/10
 - 917s - loss: 0.6678 - accuracy: 0.6326 - val_loss: 0.1572 - val_accuracy: 0.6384
Epoch 3/10
 - 920s - loss: 0.6528 - accuracy: 0.6489 - val_loss: 0.1535 - val_accuracy: 0.6461
Epoch 4/10
 - 919s - loss: 0.6446 - accuracy: 0.6565 - val_loss: 0.1616 - val_accuracy: 0.6507
Epoch 5/10
 - 918s - loss: 0.6394 - accuracy: 0.6610 - val_loss: 0.1722 - val_accuracy: 0.6537
Epoch 6/10
 - 919s - loss: 0.6352 - accuracy: 0.6643 - val_loss: 0.1821 - val_accuracy: 0.6557
Epoch 7/10
 - 918s - loss: 0.6321 - accuracy: 0.6667 - val_loss: 0.1917 - val_accuracy: 0.6572
Epoch 8/10
 - 919s - loss: 0.6294 - accuracy: 0.6687 - val_loss: 0.2011 - val_accuracy: 0.6586
Epoch 9/10
 - 917s - loss: 0.6272 - accuracy: 0.6702 - val_loss: 0.2098 - val_accuracy: 0.6596
Epoch 10/10
 - 921s - loss: 0.6252 - accuracy: 0.6716 - val_loss: 0.2178 - val_accuracy: 0.6605

Accuracy: 66.05%

Epoch 1/10
 - 1657s - loss: 0.7780 - accuracy: 0.3206 - val_loss: 0.4571 - val_accuracy: 0.4771
Epoch 2/10
 - 1658s - loss: 0.7484 - accuracy: 0.4620 - val_loss: 0.4076 - val_accuracy: 0.5190
Epoch 3/10
 - 1658s - loss: 0.7382 - accuracy: 0.4976 - val_loss: 0.3824 - val_accuracy: 0.5288
Epoch 4/10
 - 1659s - loss: 0.7320 - accuracy: 0.5158 - val_loss: 0.3656 - val_accuracy: 0.5347
Epoch 5/10
 - 1659s - loss: 0.7275 - accuracy: 0.5269 - val_loss: 0.3537 - val_accuracy: 0.5388
Epoch 6/10
 - 1658s - loss: 0.7242 - accuracy: 0.5351 - val_loss: 0.3440 - val_accuracy: 0.5417
Epoch 7/10
 - 1659s - loss: 0.7215 - accuracy: 0.5413 - val_loss: 0.3361 - val_accuracy: 0.5443
Epoch 8/10
 - 1660s - loss: 0.7190 - accuracy: 0.5462 - val_loss: 0.3295 - val_accuracy: 0.5466
Epoch 9/10
 - 1659s - loss: 0.7172 - accuracy: 0.5505 - val_loss: 0.3246 - val_accuracy: 0.5484
Epoch 10/10
 - 1659s - loss: 0.7154 - accuracy: 0.5538 - val_loss: 0.3196 - val_accuracy: 0.5499

Accuracy: 54.99%

Epoch 1/10
 - 1496s - loss: 0.7973 - accuracy: 0.1503 - val_loss: 0.6198 - val_accuracy: 0.0283
Epoch 2/10
 - 1497s - loss: 0.7872 - accuracy: 0.1920 - val_loss: 0.6271 - val_accuracy: 0.1020
Epoch 3/10
 - 1498s - loss: 0.7813 - accuracy: 0.2254 - val_loss: 0.6272 - val_accuracy: 0.2673
Epoch 4/10
 - 1499s - loss: 0.7765 - accuracy: 0.2519 - val_loss: 0.6255 - val_accuracy: 0.2827
Epoch 5/10
 - 1499s - loss: 0.7720 - accuracy: 0.2732 - val_loss: 0.6245 - val_accuracy: 0.2955
Epoch 6/10
 - 1499s - loss: 0.7672 - accuracy: 0.2931 - val_loss: 0.6224 - val_accuracy: 0.3076
Epoch 7/10
 - 1499s - loss: 0.7613 - accuracy: 0.3114 - val_loss: 0.6073 - val_accuracy: 0.3202
Epoch 8/10
 - 1499s - loss: 0.7548 - accuracy: 0.3298 - val_loss: 0.5920 - val_accuracy: 0.3338
Epoch 9/10
 - 1499s - loss: 0.7475 - accuracy: 0.3479 - val_loss: 0.5763 - val_accuracy: 0.3480
Epoch 10/10
 - 1499s - loss: 0.7400 - accuracy: 0.3658 - val_loss: 0.5746 - val_accuracy: 0.3619

Accuracy: 36.19%

Epoch 1/10
 - 1082s - loss: 0.6653 - accuracy: 0.5884 - val_loss: 0.3986 - val_accuracy: 0.6854
Epoch 2/10
 - 1085s - loss: 0.5991 - accuracy: 0.6784 - val_loss: 0.4331 - val_accuracy: 0.6774
Epoch 3/10
 - 1084s - loss: 0.5899 - accuracy: 0.6741 - val_loss: 0.4453 - val_accuracy: 0.6732
Epoch 4/10
 - 1086s - loss: 0.5851 - accuracy: 0.6719 - val_loss: 0.4499 - val_accuracy: 0.6709
Epoch 5/10
 - 1088s - loss: 0.5822 - accuracy: 0.6707 - val_loss: 0.4524 - val_accuracy: 0.6693
Epoch 6/10
 - 1099s - loss: 0.5802 - accuracy: 0.6694 - val_loss: 0.4527 - val_accuracy: 0.6683
Epoch 7/10
 - 1084s - loss: 0.5785 - accuracy: 0.6689 - val_loss: 0.4543 - val_accuracy: 0.6677
Epoch 8/10
 - 1086s - loss: 0.5773 - accuracy: 0.6684 - val_loss: 0.4554 - val_accuracy: 0.6672
Epoch 9/10
 - 1086s - loss: 0.5761 - accuracy: 0.6682 - val_loss: 0.4565 - val_accuracy: 0.6666
Epoch 10/10
 - 1086s - loss: 0.5753 - accuracy: 0.6679 - val_loss: 0.4573 - val_accuracy: 0.6661

Accuracy: 66.61%

Epoch 1/10
 - 838s - loss: 0.8198 - accuracy: 0.0761 - val_loss: 0.6606 - val_accuracy: 0.0073
Epoch 2/10
 - 841s - loss: 0.8127 - accuracy: 0.1194 - val_loss: 0.6536 - val_accuracy: 0.0131
Epoch 3/10
 - 841s - loss: 0.8119 - accuracy: 0.1347 - val_loss: 0.6504 - val_accuracy: 0.0167
Epoch 4/10
 - 840s - loss: 0.8114 - accuracy: 0.1435 - val_loss: 0.6484 - val_accuracy: 0.0194
Epoch 5/10
 - 844s - loss: 0.8112 - accuracy: 0.1493 - val_loss: 0.6472 - val_accuracy: 0.0213
Epoch 6/10
 - 841s - loss: 0.8108 - accuracy: 0.1536 - val_loss: 0.6462 - val_accuracy: 0.0229
Epoch 7/10
 - 839s - loss: 0.8108 - accuracy: 0.1570 - val_loss: 0.6454 - val_accuracy: 0.0242
Epoch 8/10
 - 841s - loss: 0.8108 - accuracy: 0.1597 - val_loss: 0.6448 - val_accuracy: 0.0253
Epoch 9/10
 - 841s - loss: 0.8106 - accuracy: 0.1622 - val_loss: 0.6443 - val_accuracy: 0.0263
Epoch 10/10
 - 841s - loss: 0.8106 - accuracy: 0.1639 - val_loss: 0.6439 - val_accuracy: 0.0271

Accuracy: 2.71%

Using TensorFlow backend.
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Epoch 1/10
 - 1835s - loss: 0.5829 - accuracy: 0.6692 - val_loss: 0.2672 - val_accuracy: 0.6811
Epoch 2/10
 - 1833s - loss: 0.5484 - accuracy: 0.6814 - val_loss: 0.2628 - val_accuracy: 0.6799
Epoch 3/10
 - 1833s - loss: 0.5357 - accuracy: 0.6854 - val_loss: 0.2485 - val_accuracy: 0.6797
Epoch 4/10
 - 1833s - loss: 0.5253 - accuracy: 0.6890 - val_loss: 0.2334 - val_accuracy: 0.6773
Epoch 5/10
 - 1832s - loss: 0.5166 - accuracy: 0.6919 - val_loss: 0.2217 - val_accuracy: 0.6743
Epoch 6/10
 - 1832s - loss: 0.5097 - accuracy: 0.6943 - val_loss: 0.2138 - val_accuracy: 0.6712
Epoch 7/10
 - 1833s - loss: 0.5039 - accuracy: 0.6964 - val_loss: 0.2059 - val_accuracy: 0.6686
Epoch 8/10
 - 1833s - loss: 0.4992 - accuracy: 0.6984 - val_loss: 0.1992 - val_accuracy: 0.6669
Epoch 9/10
 - 1833s - loss: 0.4952 - accuracy: 0.7002 - val_loss: 0.1965 - val_accuracy: 0.6648
Epoch 10/10
 - 1832s - loss: 0.4916 - accuracy: 0.7018 - val_loss: 0.1931 - val_accuracy: 0.6637

Accuracy: 66.37%

/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Epoch 1/10
 - 753s - loss: 0.5909 - accuracy: 0.6660 - val_loss: 0.4110 - val_accuracy: 0.6797
Epoch 2/10
 - 757s - loss: 0.5494 - accuracy: 0.6864 - val_loss: 0.4717 - val_accuracy: 0.6638
Epoch 3/10
 - 756s - loss: 0.5148 - accuracy: 0.7028 - val_loss: 0.4693 - val_accuracy: 0.6607
Epoch 4/10
 - 757s - loss: 0.4906 - accuracy: 0.7176 - val_loss: 0.3940 - val_accuracy: 0.6647
Epoch 5/10
 - 758s - loss: 0.4731 - accuracy: 0.7295 - val_loss: 0.3911 - val_accuracy: 0.6615
Epoch 6/10
 - 756s - loss: 0.4599 - accuracy: 0.7390 - val_loss: 0.3170 - val_accuracy: 0.6643
Epoch 7/10
 - 754s - loss: 0.4498 - accuracy: 0.7468 - val_loss: 0.3065 - val_accuracy: 0.6677
Epoch 8/10
 - 757s - loss: 0.4414 - accuracy: 0.7531 - val_loss: 0.2949 - val_accuracy: 0.6671
Epoch 9/10
 - 756s - loss: 0.4344 - accuracy: 0.7582 - val_loss: 0.2661 - val_accuracy: 0.6688
Epoch 10/10
 - 755s - loss: 0.4286 - accuracy: 0.7628 - val_loss: 0.2508 - val_accuracy: 0.6673

Accuracy: 66.73%

/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Epoch 1/10
 - 143s - loss: 0.7600 - accuracy: 0.3308 - val_loss: 0.2593 - val_accuracy: 0.4712
Epoch 2/10
 - 144s - loss: 0.6854 - accuracy: 0.4970 - val_loss: 0.2103 - val_accuracy: 0.5102
Epoch 3/10
 - 144s - loss: 0.6647 - accuracy: 0.5207 - val_loss: 0.2386 - val_accuracy: 0.5222
Epoch 4/10
 - 144s - loss: 0.6552 - accuracy: 0.5311 - val_loss: 0.2754 - val_accuracy: 0.5298
Epoch 5/10
 - 144s - loss: 0.6499 - accuracy: 0.5361 - val_loss: 0.3102 - val_accuracy: 0.5327
Epoch 6/10
 - 144s - loss: 0.6464 - accuracy: 0.5392 - val_loss: 0.3374 - val_accuracy: 0.5345
Epoch 7/10
 - 144s - loss: 0.6436 - accuracy: 0.5413 - val_loss: 0.3572 - val_accuracy: 0.5361
Epoch 8/10
 - 144s - loss: 0.6420 - accuracy: 0.5424 - val_loss: 0.3730 - val_accuracy: 0.5372
Epoch 9/10
 - 144s - loss: 0.6404 - accuracy: 0.5434 - val_loss: 0.3806 - val_accuracy: 0.5377
Epoch 10/10
 - 144s - loss: 0.6389 - accuracy: 0.5442 - val_loss: 0.3859 - val_accuracy: 0.5380

Accuracy: 53.80%

/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Epoch 1/10
 - 1835s - loss: 0.5879 - accuracy: 0.6676 - val_loss: 0.4570 - val_accuracy: 0.6798
Epoch 2/10
 - 1834s - loss: 0.5352 - accuracy: 0.6947 - val_loss: 0.6471 - val_accuracy: 0.6676
Epoch 3/10
 - 1837s - loss: 0.4858 - accuracy: 0.7219 - val_loss: 0.6750 - val_accuracy: 0.6704
Epoch 4/10
 - 1836s - loss: 0.4529 - accuracy: 0.7439 - val_loss: 0.7322 - val_accuracy: 0.6678
Epoch 5/10
 - 1837s - loss: 0.4299 - accuracy: 0.7595 - val_loss: 0.6319 - val_accuracy: 0.6733
Epoch 6/10
 - 1837s - loss: 0.4137 - accuracy: 0.7705 - val_loss: 0.5742 - val_accuracy: 0.6717
Epoch 7/10
 - 1836s - loss: 0.4013 - accuracy: 0.7781 - val_loss: 0.5931 - val_accuracy: 0.6717
Epoch 8/10
 - 1836s - loss: 0.3922 - accuracy: 0.7836 - val_loss: 0.6027 - val_accuracy: 0.6692
Epoch 9/10
 - 1836s - loss: 0.3848 - accuracy: 0.7874 - val_loss: 0.6173 - val_accuracy: 0.6701
Epoch 10/10
 - 1835s - loss: 0.3787 - accuracy: 0.7904 - val_loss: 0.6798 - val_accuracy: 0.6676

Accuracy: 66.76%

/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Epoch 1/10
 - 1051s - loss: 0.6189 - accuracy: 0.6895 - val_loss: 0.2968 - val_accuracy: 0.6983
Epoch 2/10
 - 1052s - loss: 0.5885 - accuracy: 0.7074 - val_loss: 0.3287 - val_accuracy: 0.6953
Epoch 3/10
 - 1053s - loss: 0.5587 - accuracy: 0.7286 - val_loss: 0.4485 - val_accuracy: 0.6900
Epoch 4/10
 - 1051s - loss: 0.5333 - accuracy: 0.7488 - val_loss: 0.5019 - val_accuracy: 0.6982
Epoch 5/10
 - 1052s - loss: 0.5144 - accuracy: 0.7647 - val_loss: 0.5726 - val_accuracy: 0.7101
Epoch 6/10
 - 1054s - loss: 0.5002 - accuracy: 0.7774 - val_loss: 0.6254 - val_accuracy: 0.7137
Epoch 7/10
 - 1052s - loss: 0.4895 - accuracy: 0.7867 - val_loss: 0.6218 - val_accuracy: 0.7165
Epoch 8/10
 - 1050s - loss: 0.4812 - accuracy: 0.7937 - val_loss: 0.6679 - val_accuracy: 0.7213
Epoch 9/10
 - 1051s - loss: 0.4743 - accuracy: 0.7991 - val_loss: 0.6688 - val_accuracy: 0.7233
Epoch 10/10
 - 1051s - loss: 0.4688 - accuracy: 0.8033 - val_loss: 0.6846 - val_accuracy: 0.7239

Accuracy: 72.39%

/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Epoch 1/10
 - 128s - loss: 0.7772 - accuracy: 0.1099 - val_loss: 0.6023 - val_accuracy: 0.0000e+00
Epoch 2/10
 - 129s - loss: 0.7712 - accuracy: 0.1268 - val_loss: 0.6042 - val_accuracy: 0.0226
Epoch 3/10
 - 131s - loss: 0.7699 - accuracy: 0.1268 - val_loss: 0.6055 - val_accuracy: 0.0224
Epoch 4/10
 - 131s - loss: 0.7691 - accuracy: 0.1237 - val_loss: 0.6024 - val_accuracy: 0.0280
Epoch 5/10
 - 131s - loss: 0.7686 - accuracy: 0.1184 - val_loss: 0.6037 - val_accuracy: 0.0257
Epoch 6/10
 - 130s - loss: 0.7683 - accuracy: 0.1132 - val_loss: 0.6046 - val_accuracy: 0.0272
Epoch 7/10
 - 131s - loss: 0.7678 - accuracy: 0.1097 - val_loss: 0.6045 - val_accuracy: 0.0275
Epoch 8/10
 - 131s - loss: 0.7676 - accuracy: 0.1054 - val_loss: 0.6023 - val_accuracy: 0.0287
Epoch 9/10
 - 131s - loss: 0.7675 - accuracy: 0.1020 - val_loss: 0.6051 - val_accuracy: 0.0209
Epoch 10/10
 - 131s - loss: 0.7671 - accuracy: 0.0990 - val_loss: 0.6030 - val_accuracy: 0.0260

Accuracy: 2.60%

/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Epoch 1/10
 - 1423s - loss: 0.5965 - accuracy: 0.6730 - val_loss: 0.1858 - val_accuracy: 0.6905
Epoch 2/10
 - 1428s - loss: 0.5609 - accuracy: 0.6877 - val_loss: 0.1869 - val_accuracy: 0.6875
Epoch 3/10
 - 1426s - loss: 0.5543 - accuracy: 0.6888 - val_loss: 0.1810 - val_accuracy: 0.6862
Epoch 4/10
 - 1426s - loss: 0.5499 - accuracy: 0.6898 - val_loss: 0.1753 - val_accuracy: 0.6857
Epoch 5/10
 - 1426s - loss: 0.5465 - accuracy: 0.6906 - val_loss: 0.1717 - val_accuracy: 0.6851
Epoch 6/10
 - 1426s - loss: 0.5436 - accuracy: 0.6914 - val_loss: 0.1676 - val_accuracy: 0.6849
Epoch 7/10
 - 1426s - loss: 0.5409 - accuracy: 0.6922 - val_loss: 0.1644 - val_accuracy: 0.6846
Epoch 8/10
 - 1427s - loss: 0.5387 - accuracy: 0.6925 - val_loss: 0.1607 - val_accuracy: 0.6842
Epoch 9/10
 - 1438s - loss: 0.5367 - accuracy: 0.6931 - val_loss: 0.1580 - val_accuracy: 0.6838
Epoch 10/10
 - 1424s - loss: 0.5349 - accuracy: 0.6935 - val_loss: 0.1546 - val_accuracy: 0.6834

Accuracy: 68.34%

/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Epoch 1/10
 - 1823s - loss: 0.7017 - accuracy: 0.5730 - val_loss: 0.4354 - val_accuracy: 0.6313
Epoch 2/10
 - 1824s - loss: 0.6401 - accuracy: 0.6373 - val_loss: 0.4259 - val_accuracy: 0.6312
Epoch 3/10
 - 1824s - loss: 0.6274 - accuracy: 0.6432 - val_loss: 0.4218 - val_accuracy: 0.6313
Epoch 4/10
 - 1824s - loss: 0.6204 - accuracy: 0.6460 - val_loss: 0.4207 - val_accuracy: 0.6325
Epoch 5/10
 - 1824s - loss: 0.6158 - accuracy: 0.6482 - val_loss: 0.4211 - val_accuracy: 0.6331
Epoch 6/10
 - 1824s - loss: 0.6124 - accuracy: 0.6499 - val_loss: 0.4206 - val_accuracy: 0.6340
Epoch 7/10
 - 1825s - loss: 0.6095 - accuracy: 0.6514 - val_loss: 0.4209 - val_accuracy: 0.6344
Epoch 8/10
 - 1825s - loss: 0.6074 - accuracy: 0.6526 - val_loss: 0.4203 - val_accuracy: 0.6348
Epoch 9/10
 - 1824s - loss: 0.6055 - accuracy: 0.6535 - val_loss: 0.4196 - val_accuracy: 0.6349
Epoch 10/10
 - 1823s - loss: 0.6039 - accuracy: 0.6542 - val_loss: 0.4201 - val_accuracy: 0.6352

Accuracy: 63.52%

/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Epoch 1/10
 - 1829s - loss: 0.6399 - accuracy: 0.6217 - val_loss: 0.3345 - val_accuracy: 0.7051
Epoch 2/10
 - 1817s - loss: 0.5736 - accuracy: 0.7069 - val_loss: 0.4701 - val_accuracy: 0.7027
Epoch 3/10
 - 1817s - loss: 0.5341 - accuracy: 0.7310 - val_loss: 0.5963 - val_accuracy: 0.7053
Epoch 4/10
 - 1817s - loss: 0.5048 - accuracy: 0.7541 - val_loss: 0.5565 - val_accuracy: 0.7097
Epoch 5/10
 - 1818s - loss: 0.4842 - accuracy: 0.7720 - val_loss: 0.6216 - val_accuracy: 0.7115
Epoch 6/10
 - 1818s - loss: 0.4693 - accuracy: 0.7847 - val_loss: 0.5813 - val_accuracy: 0.7131
Epoch 7/10
 - 1818s - loss: 0.4584 - accuracy: 0.7941 - val_loss: 0.5893 - val_accuracy: 0.7140
Epoch 8/10
 - 1817s - loss: 0.4504 - accuracy: 0.8006 - val_loss: 0.6007 - val_accuracy: 0.7133
Epoch 9/10
 - 1817s - loss: 0.4441 - accuracy: 0.8055 - val_loss: 0.5334 - val_accuracy: 0.7126
Epoch 10/10
 - 1818s - loss: 0.4388 - accuracy: 0.8089 - val_loss: 0.4966 - val_accuracy: 0.7126

Accuracy: 71.26%

/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Epoch 1/10
 - 390s - loss: 0.7746 - accuracy: 0.2367 - val_loss: 0.2052 - val_accuracy: 0.5867
Epoch 2/10
 - 390s - loss: 0.6921 - accuracy: 0.6094 - val_loss: 0.1876 - val_accuracy: 0.6591
Epoch 3/10
 - 394s - loss: 0.6518 - accuracy: 0.6825 - val_loss: 0.2380 - val_accuracy: 0.6867
Epoch 4/10
 - 391s - loss: 0.6328 - accuracy: 0.7009 - val_loss: 0.2829 - val_accuracy: 0.6972
Epoch 5/10
 - 394s - loss: 0.6224 - accuracy: 0.7077 - val_loss: 0.3138 - val_accuracy: 0.7018
Epoch 6/10
 - 394s - loss: 0.6156 - accuracy: 0.7108 - val_loss: 0.3286 - val_accuracy: 0.7040
Epoch 7/10
 - 394s - loss: 0.6106 - accuracy: 0.7122 - val_loss: 0.3435 - val_accuracy: 0.7051
Epoch 8/10
 - 394s - loss: 0.6069 - accuracy: 0.7127 - val_loss: 0.3526 - val_accuracy: 0.7054
Epoch 9/10
 - 394s - loss: 0.6043 - accuracy: 0.7125 - val_loss: 0.3603 - val_accuracy: 0.7053
Epoch 10/10
 - 394s - loss: 0.6021 - accuracy: 0.7125 - val_loss: 0.3673 - val_accuracy: 0.7050

Accuracy: 70.50%

Epoch 1/10
 - 955s - loss: 0.8037 - accuracy: 0.1489 - val_loss: 0.5988 - val_accuracy: 0.0923
Epoch 2/10
 - 958s - loss: 0.8020 - accuracy: 0.1613 - val_loss: 0.5996 - val_accuracy: 0.1122
Epoch 3/10
 - 958s - loss: 0.8015 - accuracy: 0.1650 - val_loss: 0.6003 - val_accuracy: 0.1229
Epoch 4/10
 - 958s - loss: 0.8011 - accuracy: 0.1675 - val_loss: 0.6007 - val_accuracy: 0.1303
Epoch 5/10
 - 960s - loss: 0.8008 - accuracy: 0.1702 - val_loss: 0.6009 - val_accuracy: 0.1360
Epoch 6/10
 - 958s - loss: 0.8006 - accuracy: 0.1720 - val_loss: 0.6010 - val_accuracy: 0.1404
Epoch 7/10
 - 958s - loss: 0.8004 - accuracy: 0.1735 - val_loss: 0.6013 - val_accuracy: 0.1441
Epoch 8/10
 - 957s - loss: 0.8002 - accuracy: 0.1746 - val_loss: 0.6013 - val_accuracy: 0.1474
Epoch 9/10
 - 958s - loss: 0.8000 - accuracy: 0.1760 - val_loss: 0.6015 - val_accuracy: 0.1500
Epoch 10/10
 - 958s - loss: 0.7999 - accuracy: 0.1771 - val_loss: 0.6016 - val_accuracy: 0.1525

Accuracy: 15.25%

Epoch 1/10
 - 774s - loss: 0.8047 - accuracy: 0.1537 - val_loss: 0.5968 - val_accuracy: 0.0639
Epoch 2/10
 - 774s - loss: 0.8036 - accuracy: 0.1612 - val_loss: 0.5964 - val_accuracy: 0.0755
Epoch 3/10
 - 775s - loss: 0.8034 - accuracy: 0.1625 - val_loss: 0.5963 - val_accuracy: 0.0828
Epoch 4/10
 - 772s - loss: 0.8030 - accuracy: 0.1634 - val_loss: 0.5964 - val_accuracy: 0.0877
Epoch 5/10
 - 774s - loss: 0.8029 - accuracy: 0.1644 - val_loss: 0.5965 - val_accuracy: 0.0915
Epoch 6/10
 - 774s - loss: 0.8028 - accuracy: 0.1651 - val_loss: 0.5965 - val_accuracy: 0.0945
Epoch 7/10
 - 775s - loss: 0.8026 - accuracy: 0.1656 - val_loss: 0.5965 - val_accuracy: 0.0970
Epoch 8/10
 - 777s - loss: 0.8026 - accuracy: 0.1661 - val_loss: 0.5967 - val_accuracy: 0.0993
Epoch 9/10
 - 774s - loss: 0.8024 - accuracy: 0.1670 - val_loss: 0.5967 - val_accuracy: 0.1014
Epoch 10/10
 - 775s - loss: 0.8023 - accuracy: 0.1670 - val_loss: 0.5967 - val_accuracy: 0.1032

Accuracy: 10.32%

Epoch 1/10
 - 1598s - loss: 0.7886 - accuracy: 0.2364 - val_loss: 0.6165 - val_accuracy: 0.2990
Epoch 2/10
 - 1597s - loss: 0.7681 - accuracy: 0.3774 - val_loss: 0.6290 - val_accuracy: 0.4532
Epoch 3/10
 - 1597s - loss: 0.7594 - accuracy: 0.4272 - val_loss: 0.6321 - val_accuracy: 0.4655
Epoch 4/10
 - 1597s - loss: 0.7539 - accuracy: 0.4512 - val_loss: 0.6186 - val_accuracy: 0.4725
Epoch 5/10
 - 1598s - loss: 0.7498 - accuracy: 0.4661 - val_loss: 0.6069 - val_accuracy: 0.4777
Epoch 6/10
 - 1597s - loss: 0.7467 - accuracy: 0.4767 - val_loss: 0.5955 - val_accuracy: 0.4816
Epoch 7/10
 - 1597s - loss: 0.7439 - accuracy: 0.4842 - val_loss: 0.5859 - val_accuracy: 0.4847
Epoch 8/10
 - 1598s - loss: 0.7418 - accuracy: 0.4904 - val_loss: 0.5767 - val_accuracy: 0.4874
Epoch 9/10
 - 1597s - loss: 0.7398 - accuracy: 0.4956 - val_loss: 0.5687 - val_accuracy: 0.4897
Epoch 10/10
 - 1598s - loss: 0.7380 - accuracy: 0.5000 - val_loss: 0.5616 - val_accuracy: 0.4919

Accuracy: 49.19%

Epoch 1/10
 - 440s - loss: 0.6282 - accuracy: 0.6665 - val_loss: 0.4171 - val_accuracy: 0.6877
Epoch 2/10
 - 438s - loss: 0.5990 - accuracy: 0.6924 - val_loss: 0.4205 - val_accuracy: 0.6857
Epoch 3/10
 - 441s - loss: 0.5946 - accuracy: 0.6950 - val_loss: 0.4043 - val_accuracy: 0.6856
Epoch 4/10
 - 442s - loss: 0.5921 - accuracy: 0.6967 - val_loss: 0.4032 - val_accuracy: 0.6863
Epoch 5/10
 - 438s - loss: 0.5900 - accuracy: 0.6981 - val_loss: 0.4014 - val_accuracy: 0.6870
Epoch 6/10
 - 442s - loss: 0.5887 - accuracy: 0.6991 - val_loss: 0.3934 - val_accuracy: 0.6873
Epoch 7/10
 - 442s - loss: 0.5875 - accuracy: 0.7000 - val_loss: 0.3916 - val_accuracy: 0.6877
Epoch 8/10
 - 441s - loss: 0.5866 - accuracy: 0.7006 - val_loss: 0.3854 - val_accuracy: 0.6878
Epoch 9/10
 - 438s - loss: 0.5859 - accuracy: 0.7011 - val_loss: 0.3816 - val_accuracy: 0.6882
Epoch 10/10
 - 441s - loss: 0.5851 - accuracy: 0.7016 - val_loss: 0.3803 - val_accuracy: 0.6884

Accuracy: 68.84%

Epoch 1/10
 - 529s - loss: 0.8077 - accuracy: 0.1900 - val_loss: 0.5965 - val_accuracy: 0.1062
Epoch 2/10
 - 529s - loss: 0.8051 - accuracy: 0.1937 - val_loss: 0.5929 - val_accuracy: 0.1149
Epoch 3/10
 - 532s - loss: 0.8042 - accuracy: 0.1941 - val_loss: 0.5890 - val_accuracy: 0.1200
Epoch 4/10
 - 531s - loss: 0.8036 - accuracy: 0.1948 - val_loss: 0.5858 - val_accuracy: 0.1233
Epoch 5/10
 - 531s - loss: 0.8031 - accuracy: 0.1949 - val_loss: 0.5831 - val_accuracy: 0.1256
Epoch 6/10
 - 532s - loss: 0.8029 - accuracy: 0.1952 - val_loss: 0.5811 - val_accuracy: 0.1276
Epoch 7/10
 - 532s - loss: 0.8026 - accuracy: 0.1954 - val_loss: 0.5796 - val_accuracy: 0.1293
Epoch 8/10
 - 532s - loss: 0.8021 - accuracy: 0.1956 - val_loss: 0.5781 - val_accuracy: 0.1307
Epoch 9/10
 - 529s - loss: 0.8020 - accuracy: 0.1961 - val_loss: 0.5768 - val_accuracy: 0.1318
Epoch 10/10
 - 528s - loss: 0.8020 - accuracy: 0.1959 - val_loss: 0.5756 - val_accuracy: 0.1330

Accuracy: 13.30%

Epoch 1/10
 - 1625s - loss: 0.6474 - accuracy: 0.6430 - val_loss: 0.2493 - val_accuracy: 0.6955
Epoch 2/10
 - 1613s - loss: 0.5956 - accuracy: 0.6914 - val_loss: 0.2845 - val_accuracy: 0.6929
Epoch 3/10
 - 1612s - loss: 0.5882 - accuracy: 0.6912 - val_loss: 0.2966 - val_accuracy: 0.6921
Epoch 4/10
 - 1614s - loss: 0.5839 - accuracy: 0.6909 - val_loss: 0.3066 - val_accuracy: 0.6917
Epoch 5/10
 - 1614s - loss: 0.5808 - accuracy: 0.6912 - val_loss: 0.3088 - val_accuracy: 0.6916
Epoch 6/10
 - 1613s - loss: 0.5787 - accuracy: 0.6912 - val_loss: 0.3127 - val_accuracy: 0.6917
Epoch 7/10
 - 1613s - loss: 0.5771 - accuracy: 0.6913 - val_loss: 0.3158 - val_accuracy: 0.6915
Epoch 8/10
 - 1613s - loss: 0.5758 - accuracy: 0.6914 - val_loss: 0.3174 - val_accuracy: 0.6913
Epoch 9/10
 - 1614s - loss: 0.5747 - accuracy: 0.6914 - val_loss: 0.3200 - val_accuracy: 0.6912
Epoch 10/10
 - 1613s - loss: 0.5739 - accuracy: 0.6915 - val_loss: 0.3210 - val_accuracy: 0.6911

Accuracy: 69.11%

Epoch 1/10
 - 1019s - loss: 0.6211 - accuracy: 0.6564 - val_loss: 0.2898 - val_accuracy: 0.6957
Epoch 2/10
 - 1023s - loss: 0.5740 - accuracy: 0.6877 - val_loss: 0.3107 - val_accuracy: 0.6886
Epoch 3/10
 - 1019s - loss: 0.5685 - accuracy: 0.6859 - val_loss: 0.3158 - val_accuracy: 0.6857
Epoch 4/10
 - 1021s - loss: 0.5655 - accuracy: 0.6852 - val_loss: 0.3155 - val_accuracy: 0.6840
Epoch 5/10
 - 1020s - loss: 0.5634 - accuracy: 0.6849 - val_loss: 0.3182 - val_accuracy: 0.6829
Epoch 6/10
 - 1021s - loss: 0.5618 - accuracy: 0.6846 - val_loss: 0.3191 - val_accuracy: 0.6822
Epoch 7/10
 - 1019s - loss: 0.5606 - accuracy: 0.6845 - val_loss: 0.3203 - val_accuracy: 0.6816
Epoch 8/10
 - 1020s - loss: 0.5595 - accuracy: 0.6844 - val_loss: 0.3195 - val_accuracy: 0.6813
Epoch 9/10
 - 1020s - loss: 0.5584 - accuracy: 0.6844 - val_loss: 0.3212 - val_accuracy: 0.6809
Epoch 10/10
 - 1020s - loss: 0.5576 - accuracy: 0.6845 - val_loss: 0.3210 - val_accuracy: 0.6805

Accuracy: 68.05%

Epoch 1/10
 - 688s - loss: 0.5836 - accuracy: 0.6620 - val_loss: 0.3508 - val_accuracy: 0.6789
Epoch 2/10
 - 689s - loss: 0.5426 - accuracy: 0.6836 - val_loss: 0.3525 - val_accuracy: 0.6759
Epoch 3/10
 - 691s - loss: 0.5227 - accuracy: 0.6912 - val_loss: 0.3754 - val_accuracy: 0.6677
Epoch 4/10
 - 689s - loss: 0.5099 - accuracy: 0.6963 - val_loss: 0.3963 - val_accuracy: 0.6618
Epoch 5/10
 - 691s - loss: 0.5015 - accuracy: 0.6998 - val_loss: 0.4050 - val_accuracy: 0.6588
Epoch 6/10
 - 690s - loss: 0.4952 - accuracy: 0.7030 - val_loss: 0.4205 - val_accuracy: 0.6571
Epoch 7/10
 - 690s - loss: 0.4902 - accuracy: 0.7054 - val_loss: 0.4252 - val_accuracy: 0.6564
Epoch 8/10
 - 691s - loss: 0.4862 - accuracy: 0.7077 - val_loss: 0.4365 - val_accuracy: 0.6556
Epoch 9/10
 - 690s - loss: 0.4827 - accuracy: 0.7096 - val_loss: 0.4410 - val_accuracy: 0.6557
Epoch 10/10
 - 692s - loss: 0.4798 - accuracy: 0.7113 - val_loss: 0.4448 - val_accuracy: 0.6557

Accuracy: 65.57%

Epoch 1/10
 - 427s - loss: 0.6210 - accuracy: 0.6534 - val_loss: 0.3357 - val_accuracy: 0.7000
Epoch 2/10
 - 432s - loss: 0.5758 - accuracy: 0.6904 - val_loss: 0.3663 - val_accuracy: 0.6903
Epoch 3/10
 - 431s - loss: 0.5704 - accuracy: 0.6866 - val_loss: 0.3720 - val_accuracy: 0.6869
Epoch 4/10
 - 430s - loss: 0.5674 - accuracy: 0.6853 - val_loss: 0.3788 - val_accuracy: 0.6852
Epoch 5/10
 - 428s - loss: 0.5655 - accuracy: 0.6845 - val_loss: 0.3803 - val_accuracy: 0.6841
Epoch 6/10
 - 430s - loss: 0.5640 - accuracy: 0.6843 - val_loss: 0.3839 - val_accuracy: 0.6835
Epoch 7/10
 - 432s - loss: 0.5629 - accuracy: 0.6843 - val_loss: 0.3868 - val_accuracy: 0.6829
Epoch 8/10
 - 431s - loss: 0.5618 - accuracy: 0.6842 - val_loss: 0.3889 - val_accuracy: 0.6825
Epoch 9/10
 - 427s - loss: 0.5608 - accuracy: 0.6844 - val_loss: 0.3927 - val_accuracy: 0.6822
Epoch 10/10
 - 431s - loss: 0.5602 - accuracy: 0.6841 - val_loss: 0.3937 - val_accuracy: 0.6819

Accuracy: 68.19%

Epoch 1/10
 - 1035s - loss: 0.7987 - accuracy: 0.1572 - val_loss: 0.6064 - val_accuracy: 0.0566
Epoch 2/10
 - 1037s - loss: 0.7903 - accuracy: 0.1993 - val_loss: 0.6077 - val_accuracy: 0.1535
Epoch 3/10
 - 1037s - loss: 0.7864 - accuracy: 0.2296 - val_loss: 0.6074 - val_accuracy: 0.1883
Epoch 4/10
 - 1037s - loss: 0.7840 - accuracy: 0.2500 - val_loss: 0.6072 - val_accuracy: 0.2096
Epoch 5/10
 - 1039s - loss: 0.7823 - accuracy: 0.2638 - val_loss: 0.6072 - val_accuracy: 0.2227
Epoch 6/10
 - 1039s - loss: 0.7810 - accuracy: 0.2741 - val_loss: 0.6069 - val_accuracy: 0.2338
Epoch 7/10
 - 1039s - loss: 0.7799 - accuracy: 0.2822 - val_loss: 0.6061 - val_accuracy: 0.2425
Epoch 8/10
 - 1038s - loss: 0.7790 - accuracy: 0.2895 - val_loss: 0.6055 - val_accuracy: 0.2500
Epoch 9/10
 - 1038s - loss: 0.7781 - accuracy: 0.2954 - val_loss: 0.6051 - val_accuracy: 0.2564
Epoch 10/10
 - 1039s - loss: 0.7774 - accuracy: 0.3004 - val_loss: 0.6042 - val_accuracy: 0.2619

Accuracy: 26.19%

Epoch 1/10
 - 315s - loss: 0.8080 - accuracy: 0.1983 - val_loss: 0.6437 - val_accuracy: 0.0826
Epoch 2/10
 - 316s - loss: 0.8046 - accuracy: 0.1942 - val_loss: 0.6509 - val_accuracy: 0.1019
Epoch 3/10
 - 319s - loss: 0.8032 - accuracy: 0.1930 - val_loss: 0.6539 - val_accuracy: 0.1106
Epoch 4/10
 - 315s - loss: 0.8025 - accuracy: 0.1929 - val_loss: 0.6561 - val_accuracy: 0.1173
Epoch 5/10
 - 314s - loss: 0.8017 - accuracy: 0.1928 - val_loss: 0.6574 - val_accuracy: 0.1227
Epoch 6/10
 - 315s - loss: 0.8013 - accuracy: 0.1932 - val_loss: 0.6585 - val_accuracy: 0.1277
Epoch 7/10
 - 315s - loss: 0.8010 - accuracy: 0.1936 - val_loss: 0.6593 - val_accuracy: 0.1318
Epoch 8/10
 - 318s - loss: 0.8007 - accuracy: 0.1938 - val_loss: 0.6601 - val_accuracy: 0.1361
Epoch 9/10
 - 317s - loss: 0.8001 - accuracy: 0.1948 - val_loss: 0.6607 - val_accuracy: 0.1394
Epoch 10/10
 - 316s - loss: 0.7997 - accuracy: 0.1952 - val_loss: 0.6611 - val_accuracy: 0.1418

Accuracy: 14.18%

Epoch 1/10
 - 372s - loss: 0.6277 - accuracy: 0.6376 - val_loss: 0.3697 - val_accuracy: 0.7041
Epoch 2/10
 - 369s - loss: 0.5840 - accuracy: 0.6919 - val_loss: 0.3838 - val_accuracy: 0.6954
Epoch 3/10
 - 375s - loss: 0.5796 - accuracy: 0.6889 - val_loss: 0.3925 - val_accuracy: 0.6921
Epoch 4/10
 - 373s - loss: 0.5771 - accuracy: 0.6880 - val_loss: 0.3950 - val_accuracy: 0.6900
Epoch 5/10
 - 374s - loss: 0.5754 - accuracy: 0.6873 - val_loss: 0.3936 - val_accuracy: 0.6888
Epoch 6/10
 - 372s - loss: 0.5739 - accuracy: 0.6870 - val_loss: 0.3936 - val_accuracy: 0.6879
Epoch 7/10
 - 370s - loss: 0.5730 - accuracy: 0.6868 - val_loss: 0.3895 - val_accuracy: 0.6872
Epoch 8/10
 - 371s - loss: 0.5721 - accuracy: 0.6869 - val_loss: 0.3909 - val_accuracy: 0.6865
Epoch 9/10
 - 374s - loss: 0.5712 - accuracy: 0.6866 - val_loss: 0.3914 - val_accuracy: 0.6861
Epoch 10/10
 - 374s - loss: 0.5707 - accuracy: 0.6867 - val_loss: 0.3925 - val_accuracy: 0.6855

Accuracy: 68.55%

Epoch 1/10
 - 270s - loss: 0.7995 - accuracy: 0.1662 - val_loss: 0.5798 - val_accuracy: 0.1533
Epoch 2/10
 - 271s - loss: 0.7891 - accuracy: 0.1876 - val_loss: 0.5576 - val_accuracy: 0.1907
Epoch 3/10
 - 271s - loss: 0.7844 - accuracy: 0.2040 - val_loss: 0.5545 - val_accuracy: 0.2052
Epoch 4/10
 - 271s - loss: 0.7809 - accuracy: 0.2154 - val_loss: 0.5539 - val_accuracy: 0.2160
Epoch 5/10
 - 270s - loss: 0.7780 - accuracy: 0.2243 - val_loss: 0.5528 - val_accuracy: 0.2260
Epoch 6/10
 - 270s - loss: 0.7754 - accuracy: 0.2321 - val_loss: 0.5496 - val_accuracy: 0.2342
Epoch 7/10
 - 269s - loss: 0.7731 - accuracy: 0.2392 - val_loss: 0.5470 - val_accuracy: 0.2415
Epoch 8/10
 - 270s - loss: 0.7711 - accuracy: 0.2456 - val_loss: 0.5451 - val_accuracy: 0.2479
Epoch 9/10
 - 269s - loss: 0.7689 - accuracy: 0.2516 - val_loss: 0.5428 - val_accuracy: 0.2539
Epoch 10/10
 - 270s - loss: 0.7669 - accuracy: 0.2575 - val_loss: 0.5386 - val_accuracy: 0.2597

Accuracy: 25.97%

Epoch 1/10
 - 1078s - loss: 0.5901 - accuracy: 0.6140 - val_loss: 0.2412 - val_accuracy: 0.6515
Epoch 2/10
 - 1079s - loss: 0.5589 - accuracy: 0.6367 - val_loss: 0.2570 - val_accuracy: 0.6461
Epoch 3/10
 - 1078s - loss: 0.5532 - accuracy: 0.6391 - val_loss: 0.2583 - val_accuracy: 0.6445
Epoch 4/10
 - 1080s - loss: 0.5492 - accuracy: 0.6412 - val_loss: 0.2619 - val_accuracy: 0.6450
Epoch 5/10
 - 1080s - loss: 0.5465 - accuracy: 0.6428 - val_loss: 0.2648 - val_accuracy: 0.6457
Epoch 6/10
 - 1081s - loss: 0.5439 - accuracy: 0.6443 - val_loss: 0.2667 - val_accuracy: 0.6462
Epoch 7/10
 - 1080s - loss: 0.5421 - accuracy: 0.6451 - val_loss: 0.2679 - val_accuracy: 0.6460
Epoch 8/10
 - 1079s - loss: 0.5400 - accuracy: 0.6458 - val_loss: 0.2703 - val_accuracy: 0.6460
Epoch 9/10
 - 1078s - loss: 0.5384 - accuracy: 0.6468 - val_loss: 0.2690 - val_accuracy: 0.6455
Epoch 10/10
 - 1080s - loss: 0.5369 - accuracy: 0.6472 - val_loss: 0.2704 - val_accuracy: 0.6450

Accuracy: 64.50%

Epoch 1/10
 - 153s - loss: 0.7990 - accuracy: 0.0483 - val_loss: 0.6161 - val_accuracy: 0.0000e+00
Epoch 2/10
 - 153s - loss: 0.7967 - accuracy: 0.0651 - val_loss: 0.6236 - val_accuracy: 0.0127
Epoch 3/10
 - 153s - loss: 0.7958 - accuracy: 0.0857 - val_loss: 0.6277 - val_accuracy: 0.0349
Epoch 4/10
 - 153s - loss: 0.7951 - accuracy: 0.1000 - val_loss: 0.6295 - val_accuracy: 0.0487
Epoch 5/10
 - 153s - loss: 0.7946 - accuracy: 0.1118 - val_loss: 0.6301 - val_accuracy: 0.0597
Epoch 6/10
 - 153s - loss: 0.7941 - accuracy: 0.1215 - val_loss: 0.6307 - val_accuracy: 0.0665
Epoch 7/10
 - 154s - loss: 0.7938 - accuracy: 0.1286 - val_loss: 0.6313 - val_accuracy: 0.0718
Epoch 8/10
 - 154s - loss: 0.7935 - accuracy: 0.1351 - val_loss: 0.6318 - val_accuracy: 0.0760
Epoch 9/10
 - 153s - loss: 0.7932 - accuracy: 0.1402 - val_loss: 0.6317 - val_accuracy: 0.0787
Epoch 10/10
 - 154s - loss: 0.7929 - accuracy: 0.1449 - val_loss: 0.6317 - val_accuracy: 0.0957

Accuracy: 9.57%

Epoch 1/10
 - 1125s - loss: 0.6378 - accuracy: 0.6420 - val_loss: 0.2274 - val_accuracy: 0.6966
Epoch 2/10
 - 1125s - loss: 0.5799 - accuracy: 0.6871 - val_loss: 0.2625 - val_accuracy: 0.6879
Epoch 3/10
 - 1125s - loss: 0.5726 - accuracy: 0.6854 - val_loss: 0.2788 - val_accuracy: 0.6849
Epoch 4/10
 - 1124s - loss: 0.5691 - accuracy: 0.6847 - val_loss: 0.2864 - val_accuracy: 0.6829
Epoch 5/10
 - 1124s - loss: 0.5668 - accuracy: 0.6843 - val_loss: 0.2887 - val_accuracy: 0.6820
Epoch 6/10
 - 1123s - loss: 0.5649 - accuracy: 0.6843 - val_loss: 0.2927 - val_accuracy: 0.6811
Epoch 7/10
 - 1125s - loss: 0.5635 - accuracy: 0.6841 - val_loss: 0.2936 - val_accuracy: 0.6804
Epoch 8/10
 - 1126s - loss: 0.5623 - accuracy: 0.6840 - val_loss: 0.2964 - val_accuracy: 0.6798
Epoch 9/10
 - 1126s - loss: 0.5613 - accuracy: 0.6840 - val_loss: 0.2988 - val_accuracy: 0.6792
Epoch 10/10
 - 1126s - loss: 0.5603 - accuracy: 0.6839 - val_loss: 0.3005 - val_accuracy: 0.6789

Accuracy: 67.89%

Epoch 1/10
 - 363s - loss: 0.5966 - accuracy: 0.6557 - val_loss: 0.2921 - val_accuracy: 0.6780
Epoch 2/10
 - 360s - loss: 0.5620 - accuracy: 0.6743 - val_loss: 0.2968 - val_accuracy: 0.6726
Epoch 3/10
 - 362s - loss: 0.5572 - accuracy: 0.6749 - val_loss: 0.3000 - val_accuracy: 0.6706
Epoch 4/10
 - 364s - loss: 0.5545 - accuracy: 0.6754 - val_loss: 0.3047 - val_accuracy: 0.6698
Epoch 5/10
 - 365s - loss: 0.5525 - accuracy: 0.6761 - val_loss: 0.3048 - val_accuracy: 0.6692
Epoch 6/10
 - 364s - loss: 0.5510 - accuracy: 0.6764 - val_loss: 0.3040 - val_accuracy: 0.6686
Epoch 7/10
 - 364s - loss: 0.5497 - accuracy: 0.6770 - val_loss: 0.3049 - val_accuracy: 0.6680
Epoch 8/10
 - 364s - loss: 0.5485 - accuracy: 0.6772 - val_loss: 0.3076 - val_accuracy: 0.6677
Epoch 9/10
 - 364s - loss: 0.5476 - accuracy: 0.6775 - val_loss: 0.3086 - val_accuracy: 0.6671
Epoch 10/10
 - 364s - loss: 0.5468 - accuracy: 0.6777 - val_loss: 0.3081 - val_accuracy: 0.6669

Accuracy: 66.69%

Epoch 1/10
 - 323s - loss: 0.6319 - accuracy: 0.6274 - val_loss: 0.3570 - val_accuracy: 0.6901
Epoch 2/10
 - 322s - loss: 0.5745 - accuracy: 0.6805 - val_loss: 0.3727 - val_accuracy: 0.6790
Epoch 3/10
 - 321s - loss: 0.5690 - accuracy: 0.6769 - val_loss: 0.3756 - val_accuracy: 0.6752
Epoch 4/10
 - 320s - loss: 0.5662 - accuracy: 0.6758 - val_loss: 0.3764 - val_accuracy: 0.6736
Epoch 5/10
 - 321s - loss: 0.5644 - accuracy: 0.6753 - val_loss: 0.3780 - val_accuracy: 0.6726
Epoch 6/10
 - 321s - loss: 0.5630 - accuracy: 0.6752 - val_loss: 0.3769 - val_accuracy: 0.6718
Epoch 7/10
 - 335s - loss: 0.5620 - accuracy: 0.6751 - val_loss: 0.3772 - val_accuracy: 0.6713
Epoch 8/10
 - 321s - loss: 0.5612 - accuracy: 0.6750 - val_loss: 0.3775 - val_accuracy: 0.6710
Epoch 9/10
 - 321s - loss: 0.5605 - accuracy: 0.6750 - val_loss: 0.3774 - val_accuracy: 0.6707
Epoch 10/10
 - 321s - loss: 0.5598 - accuracy: 0.6750 - val_loss: 0.3771 - val_accuracy: 0.6705

Accuracy: 67.05%

Epoch 1/10
 - 143s - loss: 0.7903 - accuracy: 0.1259 - val_loss: 0.5230 - val_accuracy: 0.2672
Epoch 2/10
 - 143s - loss: 0.7573 - accuracy: 0.3118 - val_loss: 0.4505 - val_accuracy: 0.3911
Epoch 3/10
 - 143s - loss: 0.7371 - accuracy: 0.3704 - val_loss: 0.4310 - val_accuracy: 0.4340
Epoch 4/10
 - 143s - loss: 0.7255 - accuracy: 0.3985 - val_loss: 0.4235 - val_accuracy: 0.4559
Epoch 5/10
 - 143s - loss: 0.7184 - accuracy: 0.4162 - val_loss: 0.4260 - val_accuracy: 0.4696
Epoch 6/10
 - 143s - loss: 0.7133 - accuracy: 0.4280 - val_loss: 0.4277 - val_accuracy: 0.4787
Epoch 7/10
 - 144s - loss: 0.7095 - accuracy: 0.4365 - val_loss: 0.4316 - val_accuracy: 0.4854
Epoch 8/10
 - 144s - loss: 0.7064 - accuracy: 0.4427 - val_loss: 0.4328 - val_accuracy: 0.4902
Epoch 9/10
 - 143s - loss: 0.7042 - accuracy: 0.4477 - val_loss: 0.4311 - val_accuracy: 0.4942
Epoch 10/10
 - 143s - loss: 0.7021 - accuracy: 0.4519 - val_loss: 0.4293 - val_accuracy: 0.4973
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
/home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/conda/c9bf1e46/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "

Accuracy: 49.73%

PLOT GENERATION
SAVE RESULTS
result of gaussian process hyperparameter optimization
[(-0.7238566279411316, [289, 0.003949418198474033, 1e-06, 16]), (-0.712600588798523, [500, 0.0001, 1e-06, 16]), (-0.6907740235328674, [100, 0.001, 0.001, 64]), (-0.6833831071853638, [388, 0.002508776129648426, 0.0013380154725966922, 266]), (-0.6681973338127136, [50, 0.0011766335553050004, 0.0009354889904273485, 308]), (-0.6676140427589417, [500, 0.0016538884658827907, 1e-06, 512]), (-0.6673123240470886, [200, 0.0019929019026585434, 1e-06, 512]), (-0.6661351323127747, [291, 0.001062489858087984, 0.0039926428990265895, 341]), (-0.6637229323387146, [500, 0.01, 0.0009549219473379615, 512]), (-0.6604613065719604, [249, 0.005042634148540531, 0.009276340516226413, 150]), (-0.6553900837898254, [222, 0.0001769481562043732, 5.963542269462318e-05, 422]), (-0.6352381706237793, [500, 0.01, 0.01, 512]), (-0.6224125623703003, [494, 0.0010316744119723306, 0.003638018983374602, 342]), (-0.5499372482299805, [450, 0.0014344577276723493, 0.00811353361785334, 478]), (-0.5396154522895813, [416, 0.0016770755872644576, 0.006823577250980135, 165]), (-0.5380439162254333, [5, 0.01, 0.005909875542691159, 512]), (-0.3619302213191986, [406, 0.0007601689785474947, 0.005202271698056154, 195]), (-0.14921395480632782, [431, 0.0003328825186301368, 0.009602150207667415, 482]), (-0.027096416801214218, [230, 0.00016065700742228092, 0.009578992237269301, 43]), (-0.025973225012421608, [5, 0.01, 1e-06, 16])]
result of gradient boosted regression trees hyperparameter optimization
[(-0.7050341963768005, [100, 0.001, 0.001, 64]), (-0.6910680532455444, [438, 0.005539784628578991, 0.008026906167497917, 229]), (-0.6884061694145203, [112, 0.008743407316600383, 0.0018406373966085036, 25]), (-0.6854906678199768, [94, 0.001502092167905688, 0.0009888444432197064, 38]), (-0.6818906664848328, [107, 0.001457566123774515, 0.0011510996877804555, 141]), (-0.6804628372192383, [276, 0.009395846279758127, 0.004553605974007253, 385]), (-0.6789153814315796, [299, 0.009412497694139843, 0.005300172227505644, 501]), (-0.6705449819564819, [74, 0.0011098890725137415, 0.0009385882134153074, 390]), (-0.6669420599937439, [83, 0.0025026778304120968, 0.0008454128456229082, 474]), (-0.65566486120224, [184, 0.0014960703042385883, 0.00017039798468280104, 260]), (-0.645012378692627, [298, 0.000960697951954729, 0.0010005813455701137, 33]), (-0.497319757938385, [12, 0.0010805333372473933, 0.00046445249328015746, 21]), (-0.49191784858703613, [430, 0.0010942097424421569, 0.006929083474586539, 481]), (-0.26189860701560974, [282, 0.0006934893021691861, 0.004174567304085685, 389]), (-0.2597351372241974, [63, 0.0010696439978878142, 0.002255070097764173, 65]), (-0.152483269572258, [258, 0.00021376848315130673, 0.00504263972899721, 493]), (-0.14181534945964813, [77, 0.0008951389381611875, 0.0073690934705811905, 70]), (-0.13301299512386322, [142, 0.0004743494905152085, 0.008338967670342406, 131]), (-0.10320837795734406, [207, 0.0002882339710352859, 0.007044606545564541, 422]), (-0.0956767201423645, [16, 0.005377467222594655, 0.007247675037456357, 49])]
[Tue Mar 31 22:52:44 2020]
Finished job 2.
4 of 5 steps (80%) done

[Tue Mar 31 22:52:44 2020]
localrule all:
    input: results/hyperopt_gp.csv, results/hyperopt_gbrt.csv, results/conv_gp.png, results/conv_gbrt.png, results/obj_gp.png, results/obj_gbrt.png
    jobid: 1

[Tue Mar 31 22:52:44 2020]
Finished job 1.
5 of 5 steps (100%) done
Complete log: /home/hroetsc/Documents/ProtTransEmbedding/Snakemake/optimization/.snakemake/log/2020-03-27T151550.377207.snakemake.log
